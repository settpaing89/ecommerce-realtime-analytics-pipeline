# Architecture Documentation

Detailed technical architecture and design decisions for the E-Commerce Analytics Pipeline.

---

## ğŸ¯ Design Principles

### 1. Serverless-First
**Why:** Zero idle costs, automatic scaling, pay-per-use

| Service | Traditional | Serverless | Savings |
|---------|------------|------------|---------|
| Compute | EC2 24/7 | Lambda on-demand | ~$50/month |
| Database | RDS 24/7 | Athena queries | ~$30/month |
| ETL | EMR cluster | Glue jobs | ~$100/month |

### 2. Medallion Architecture
**Why:** Progressive data quality, clear ownership, reusable layers

```
Bronze (Raw)  â†’  Silver (Validated)  â†’  Gold (Business)
â””â”€ Immutable      â””â”€ Quality-checked      â””â”€ Aggregated
```

### 3. Infrastructure as Code
**Why:** Reproducible, version-controlled, testable infrastructure

```hcl
# One command deploys everything
terraform apply

# Modular design
modules/
â”œâ”€â”€ s3/        # Reusable across projects
â”œâ”€â”€ lambda/    # Easy to update
â””â”€â”€ glue/      # Independent testing
```

---

## ğŸ—ï¸ System Architecture

### High-Level Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      INGESTION LAYER                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Data Sources  â†’  Lambda (Validation)  â†’  S3 Bronze         â”‚
â”‚  â”œâ”€ API calls      â”œâ”€ Schema check         â””â”€ Raw parquet  â”‚
â”‚  â”œâ”€ File uploads   â”œâ”€ Type conversion                       â”‚
â”‚  â””â”€ Events         â””â”€ Error handling                        â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     PROCESSING LAYER                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Glue Crawler  â†’  Data Catalog  â†’  Glue ETL                â”‚
â”‚  â””â”€ Discover       â””â”€ Metadata      â”œâ”€ PySpark transform   â”‚
â”‚     schema                           â””â”€ S3 Silver/Gold      â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      ANALYTICS LAYER                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Athena (SQL)  â†’  Results  â†’  Dashboards                   â”‚
â”‚  â””â”€ Serverless     â””â”€ S3      â”œâ”€ Tableau Public            â”‚
â”‚     queries                    â””â”€ Apache Superset           â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Component Interactions

```mermaid
sequenceDiagram
    participant User
    participant Lambda
    participant S3
    participant Glue
    participant Athena
    
    User->>Lambda: Upload data
    Lambda->>Lambda: Validate schema
    Lambda->>S3: Write to bronze/
    Lambda->>User: Return success
    
    User->>Glue: Start crawler
    Glue->>S3: Read bronze/
    Glue->>Glue: Discover schema
    Glue->>Glue: Update catalog
    
    User->>Athena: Run SQL query
    Athena->>Glue: Get table metadata
    Athena->>S3: Read parquet
    Athena->>S3: Write results
    Athena->>User: Return data
```

---

## ğŸ“Š Data Flow Architecture

### Detailed Data Journey

```
1. INGESTION (Lambda)
   â”œâ”€ Input: JSON/CSV files
   â”œâ”€ Validation: Schema + business rules
   â”œâ”€ Enrichment: Add timestamps, source metadata
   â””â”€ Output: Parquet to S3 bronze/
       â””â”€ Partitioned by: year/month/day

2. CATALOGING (Glue Crawler)
   â”œâ”€ Input: S3 bronze/ files
   â”œâ”€ Process: Infer schema from parquet
   â”œâ”€ Output: Table metadata in Glue Catalog
   â””â”€ Frequency: On-demand or scheduled

3. TRANSFORMATION (Glue ETL / Python)
   â”œâ”€ Input: Bronze layer
   â”œâ”€ Process:
   â”‚   â”œâ”€ Deduplicate records
   â”‚   â”œâ”€ Type conversions
   â”‚   â”œâ”€ Join operations
   â”‚   â””â”€ Aggregations
   â”œâ”€ Output: Silver (cleaned) + Gold (metrics)
   â””â”€ Format: Parquet with compression

4. ANALYTICS (Athena)
   â”œâ”€ Input: SQL queries
   â”œâ”€ Process: Scan S3 parquet files
   â”œâ”€ Output: Query results to S3
   â””â”€ Cost: $5 per TB scanned
```

### Data Partitioning Strategy

```
s3://bucket/customers/
â”œâ”€â”€ year=2025/
â”‚   â”œâ”€â”€ month=01/
â”‚   â”‚   â”œâ”€â”€ day=01/
â”‚   â”‚   â”‚   â””â”€â”€ customers.parquet
â”‚   â”‚   â”œâ”€â”€ day=02/
â”‚   â”‚   â”‚   â””â”€â”€ customers.parquet
â”‚   â”‚   â””â”€â”€ day=27/
â”‚   â”‚       â””â”€â”€ customers.parquet  â† Current
â”‚   â””â”€â”€ month=02/
â”‚       â””â”€â”€ ...

Benefits:
âœ“ Athena queries only scan relevant partitions
âœ“ Easy to delete old data
âœ“ Supports incremental loads
âœ“ Clear data lineage
```

---

## ğŸ—„ï¸ Data Model

### Bronze Layer (Raw)

**Purpose:** Immutable source of truth

```
customers/
â”œâ”€â”€ Schema: As received from source
â”œâ”€â”€ Quality: No validation
â”œâ”€â”€ Format: Parquet (snappy compression)
â””â”€â”€ Retention: 1 year â†’ Glacier â†’ Delete

orders/
â”œâ”€â”€ Schema: Original order structure
â”œâ”€â”€ Duplicates: Possible
â””â”€â”€ Nulls: Allowed

products/
events/
```

### Silver Layer (Validated)

**Purpose:** Cleaned, business-ready data

```
customers_clean/
â”œâ”€â”€ Schema: Standardized
â”œâ”€â”€ Quality: 
â”‚   âœ“ No duplicates
â”‚   âœ“ Valid emails
â”‚   âœ“ Proper types
â””â”€â”€ Transformations:
    â”œâ”€ Email â†’ lowercase
    â”œâ”€ Phone â†’ standard format
    â””â”€ Nulls â†’ defaults

orders_validated/
â”œâ”€â”€ Quality:
â”‚   âœ“ Valid customer_id (FK)
â”‚   âœ“ Valid product_id (FK)
â”‚   âœ“ Amount > 0
â””â”€ Enriched with: order_year, order_month
```

### Gold Layer (Analytics)

**Purpose:** Pre-aggregated business metrics

```
daily_sales_summary/
â”œâ”€â”€ Granularity: Daily
â”œâ”€â”€ Metrics:
â”‚   â”œâ”€ total_orders
â”‚   â”œâ”€ total_revenue
â”‚   â”œâ”€ avg_order_value
â”‚   â””â”€ unique_customers
â””â”€â”€ Dimensions: date, product_category, region

customer_lifetime_value/
â”œâ”€â”€ Granularity: Customer
â”œâ”€â”€ Metrics:
â”‚   â”œâ”€ total_orders
â”‚   â”œâ”€ total_spent
â”‚   â”œâ”€ avg_order_value
â”‚   â”œâ”€ days_since_first_order
â”‚   â””â”€ rfm_segment
â””â”€â”€ Updated: Daily

product_performance/
conversion_funnel/
```

---

## âš™ï¸ Infrastructure Components

### AWS S3 (Storage)

```hcl
# Bronze Bucket Configuration
resource "aws_s3_bucket" "bronze" {
  bucket = "ecommerce-analytics-dev-bronze-${account_id}"
  
  versioning {
    enabled = true  # Protect against accidental deletion
  }
  
  lifecycle_rule {
    enabled = true
    
    transition {
      days          = 30
      storage_class = "INTELLIGENT_TIERING"  # Auto-optimize costs
    }
    
    transition {
      days          = 90
      storage_class = "GLACIER"  # Long-term archive
    }
    
    expiration {
      days = 365  # Delete after 1 year
    }
  }
  
  server_side_encryption_configuration {
    rule {
      apply_server_side_encryption_by_default {
        sse_algorithm = "AES256"  # Encrypt at rest
      }
    }
  }
}
```

**Why these settings:**
- Versioning: Recover deleted files
- Lifecycle: Automatic cost optimization
- Encryption: Security compliance

### AWS Lambda (Compute)

```python
# Lambda Function Specs
Runtime: Python 3.11
Memory: 256 MB
Timeout: 60 seconds
Trigger: S3 event / API Gateway

# Cost calculation:
# 100 invocations Ã— 2 seconds Ã— 256MB = 51.2 GB-seconds
# Free tier: 400,000 GB-seconds/month
# Usage: 0.013% of free tier = $0.00
```

**Design decisions:**
- Small memory (256MB): Our data is <10MB per batch
- Short timeout (60s): Fast fail for debugging
- Event-driven: Only runs when data arrives

### AWS Glue (Catalog & ETL)

```yaml
Glue Crawler:
  Schedule: On-demand (manual trigger)
  Target: s3://bronze-bucket/
  Frequency: After data upload
  Cost: $0.00 (within 1M objects free tier)

Glue Data Catalog:
  Tables: 4 (customers, products, orders, events)
  Partitions: ~30 (daily for 1 month)
  Cost: $0.00 (within 1M objects free tier)

Glue ETL Jobs (Optional):
  Type: Python Shell (not Spark for small data)
  DPU: 0.0625 (1/16 DPU)
  Runtime: ~1 minute per run
  Cost: $0.44/hour Ã— 1/60 = $0.007 per run
```

**Why Glue over EMR:**
- No cluster management
- Auto-scaling built-in
- Integrated with S3/Athena
- Cheaper for small workloads

### Amazon Athena (Query)

```sql
-- Query performance optimization
CREATE TABLE orders_optimized
WITH (
  format = 'PARQUET',
  parquet_compression = 'SNAPPY',
  partitioned_by = ARRAY['year', 'month'],
  bucketed_by = ARRAY['customer_id'],
  bucket_count = 10
) AS
SELECT * FROM orders;

-- Cost calculation:
-- 100 queries Ã— 100MB scanned = 10GB total
-- $5 per TB = $5/1000 Ã— 10 = $0.05
```

**Optimization techniques:**
- Parquet format: 10x faster than JSON
- Compression: 5x smaller files
- Partitioning: Skip irrelevant data
- Bucketing: Co-locate related data

---

## ğŸ”„ Workflow Orchestration

### Airflow DAG Architecture

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'data-engineering',
    'depends_on_past': False,
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'ecommerce_pipeline',
    default_args=default_args,
    description='Daily e-commerce ETL pipeline',
    schedule_interval='0 2 * * *',  # 2 AM daily
    start_date=datetime(2025, 1, 1),
    catchup=False,
    tags=['production', 'ecommerce'],
)

# Task dependency graph:
generate_data >> validate_data >> upload_s3 >> run_crawler >> 
wait_crawler >> transform_silver >> transform_gold >> 
data_quality_check >> send_notification
```

**Why Airflow:**
- Visual monitoring (DAG UI)
- Built-in retry logic
- Email alerts on failure
- Task parallelization
- Historical run tracking

---

## ğŸ” Security Architecture

### Identity & Access Management

```
IAM Hierarchy:
â”œâ”€â”€ Lambda Execution Role
â”‚   â”œâ”€â”€ S3: PutObject, GetObject (bronze bucket only)
â”‚   â”œâ”€â”€ CloudWatch: CreateLogStream, PutLogEvents
â”‚   â””â”€â”€ X-Ray: PutTraceSegments
â”‚
â”œâ”€â”€ Glue Crawler Role
â”‚   â”œâ”€â”€ S3: GetObject, ListBucket (all buckets)
â”‚   â”œâ”€â”€ Glue: CreateTable, UpdateTable, DeleteTable
â”‚   â””â”€â”€ CloudWatch: PutMetricData
â”‚
â””â”€â”€ Athena Query Role
    â”œâ”€â”€ S3: GetObject (all buckets), PutObject (gold only)
    â”œâ”€â”€ Glue: GetTable, GetDatabase
    â””â”€â”€ CloudWatch: PutMetricData
```

**Principle of Least Privilege:**
- Each service only has permissions it needs
- No wildcard (*) permissions
- Read-only where possible
- Separate roles per function

### Network Security

```
Data Flow Security:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Public    â”‚
â”‚  Internet   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ HTTPS only
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  API Gatewayâ”‚  â† IAM authentication
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Lambda    â”‚  â† Isolated execution
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  S3 (VPC    â”‚  â† Private buckets
â”‚  Endpoint)  â”‚  â† Encryption at rest
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Future enhancement:** VPC endpoints for private S3 access

---

## ğŸ“Š Monitoring & Observability

### CloudWatch Dashboards

```json
{
  "widgets": [
    {
      "type": "metric",
      "properties": {
        "title": "Lambda Invocations",
        "metrics": [
          ["AWS/Lambda", "Invocations", {"stat": "Sum"}],
          [".", "Errors", {"stat": "Sum"}],
          [".", "Duration", {"stat": "Average"}]
        ]
      }
    },
    {
      "type": "metric",
      "properties": {
        "title": "S3 Storage",
        "metrics": [
          ["AWS/S3", "BucketSizeBytes", {"stat": "Average"}],
          [".", "NumberOfObjects", {"stat": "Average"}]
        ]
      }
    }
  ]
}
```

### Key Metrics

| Metric | Threshold | Alert |
|--------|-----------|-------|
| Lambda errors | >1% | Email |
| Lambda duration | >30s | Slack |
| S3 storage | >1GB | Dashboard |
| Athena failures | >0 | Email |
| Cost | >$4 | Email + SMS |

### Logging Strategy

```
Log Levels:
â”œâ”€â”€ ERROR: Lambda validation failures, ETL errors
â”œâ”€â”€ WARN:  Data quality issues, performance degradation
â”œâ”€â”€ INFO:  Successful completions, record counts
â””â”€â”€ DEBUG: Detailed execution (dev only)

Log Retention:
â”œâ”€â”€ Lambda: 7 days
â”œâ”€â”€ Glue: 30 days
â””â”€â”€ Application: 90 days

Log Analysis:
CloudWatch Insights queries for:
- Error trends
- Performance metrics
- Data volume tracking
```

---

## ğŸ’° Cost Architecture

### Cost Breakdown by Layer

```
Monthly Costs (if left running):

Ingestion Layer:
â”œâ”€â”€ Lambda: $0.00 (free tier: 1M requests)
â””â”€â”€ API Gateway: $0.00 (not deployed)

Storage Layer:
â”œâ”€â”€ S3 Bronze: $0.00 (within 5GB free tier)
â”œâ”€â”€ S3 Silver: $0.00 (within 5GB free tier)
â””â”€â”€ S3 Gold: $0.00 (within 5GB free tier)

Processing Layer:
â”œâ”€â”€ Glue Catalog: $0.00 (within 1M objects)
â”œâ”€â”€ Glue Crawler: $0.00 (within 1M objects)
â””â”€â”€ Glue ETL: $0.07 (2 runs Ã— $0.035)

Analytics Layer:
â”œâ”€â”€ Athena: $0.50 (100MB Ã— 100 queries)
â””â”€â”€ Redshift: $0.00 (not deployed)

Monitoring:
â”œâ”€â”€ CloudWatch Logs: $0.00 (within 5GB)
â””â”€â”€ CloudWatch Metrics: $0.00 (within 10 custom)

TOTAL: $0.57/month (without Redshift demo)
```

### Cost Optimization Techniques

1. **S3 Lifecycle Policies**
   ```
   Day 1-30:  Standard storage
   Day 31-90: Intelligent Tiering (save 30%)
   Day 91+:   Glacier (save 90%)
   Day 365+:  Delete
   ```

2. **Athena Query Optimization**
   ```sql
   -- BAD: Scans entire table
   SELECT * FROM orders;
   
   -- GOOD: Uses partitions
   SELECT * FROM orders 
   WHERE year=2025 AND month=01;
   
   -- BETTER: Columnar selection
   SELECT order_id, total_amount 
   FROM orders 
   WHERE year=2025 AND month=01;
   ```

3. **Lambda Right-Sizing**
   ```
   Tested memory sizes:
   128MB: 4.2s execution = $0.000088
   256MB: 2.1s execution = $0.000044 âœ“ Optimal
   512MB: 1.9s execution = $0.000079
   ```

---

## ğŸš€ Scalability Architecture

### Current Capacity

| Component | Current | Max Capacity | Bottleneck |
|-----------|---------|--------------|------------|
| Lambda | 100/day | 1,000/second | None |
| S3 | 4MB | Unlimited | None |
| Glue | 4 tables | 1M objects | None |
| Athena | 10 queries | Unlimited | Query concurrency (20) |

### Scaling Strategies

**Horizontal Scaling (More Resources):**
```
Current: 1 Lambda function
Scale to: Multiple functions by data type
â”œâ”€â”€ lambda-customer-ingest
â”œâ”€â”€ lambda-product-ingest
â””â”€â”€ lambda-order-ingest
```

**Vertical Scaling (Bigger Resources):**
```
Current: Lambda 256MB
Scale to: Lambda 1024MB (4x faster)
Trade-off: 4x cost, but worth it if >1000 req/min
```

**Partitioning Strategy:**
```
Current: Daily partitions
Scale to: Hourly partitions
year=2025/month=01/day=27/hour=14/
â””â”€ Better query performance
â””â”€ Finer-grained data management
```

---

## ğŸ”„ Disaster Recovery

### Backup Strategy

```
S3 Versioning: Enabled
â””â”€ Recover deleted objects within 30 days

Cross-Region Replication: Not enabled (future)
â””â”€ Replicate to us-west-2 for disaster recovery

Terraform State:
â”œâ”€â”€ Local: terraform.tfstate (backed up to Git)
â””â”€â”€ Remote: S3 backend with versioning (future)

Data Retention:
â”œâ”€â”€ Bronze: 1 year (then deleted)
â”œâ”€â”€ Silver: 6 months (then archived)
â””â”€â”€ Gold: 2 years (business requirements)
```

### Recovery Procedures

```bash
# Scenario: Accidental table deletion
aws glue get-table --database ecommerce_analytics_dev --name orders
# Error: Table not found

# Recovery:
aws glue start-crawler --name $GLUE_CRAWLER
# Crawler recreates table from S3 data

# Scenario: Infrastructure destroyed
cd terraform
terraform apply  # Recreates everything
./scripts/upload_to_s3.sh  # Re-uploads data (if local copy exists)
```

---

## ğŸ“ˆ Future Architecture

### Phase 2 Enhancements

```
Current:            Future:
Batch ETL     â†’     Real-time streaming
Lambda        â†’     Lambda + Kinesis
Athena        â†’     Athena + Redshift Spectrum
Manual        â†’     Automated (EventBridge + Step Functions)
```

### Streaming Architecture (Future)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Event Source â”‚ (Website clicks, transactions)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Kinesis      â”‚ Real-time data stream
â”‚ Data Stream  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â†’ Lambda (Real-time processing)
       â”‚   â””â”€â†’ S3 Bronze
       â”‚
       â””â”€â†’ Kinesis Firehose (Batch delivery)
           â””â”€â†’ S3 Bronze

Benefits:
âœ“ Sub-second latency
âœ“ Real-time dashboards
âœ“ Immediate alerts
Cost: +$1.50/month
```

---

**Architecture Questions?** See [Troubleshooting Guide](troubleshooting.md) or open an issue.